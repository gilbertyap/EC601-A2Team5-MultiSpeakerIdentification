# Sprint 3

## Summary

In Sprint 3, our goal was to finalize several of our software libraries. We set out to experiment with and test the machine learning portions of `pyannote-audio` and `lip-reading-deeplearning`, our software for performing speaker identification and lip tracking respectively.  We also used our time during Sprint 3 to search for audio-video datasets with speaker timestamps to test our system against.

Prior to Sprint 3, we had already gotten `pyannote-audio` working on BU's SCC and were moving forward with training on a custom dataset. The test dataset chosen for this task was the `VoxConverse` dataset, a collection of audio files that contained multiple speakers mainly consisting of news segments and political debates. More details about the training can be found in the folder `audioOnlyTesting`. In summary, we were able to train 2 out of 3 of the modules in `pyannote-audio`: the Speech Activity Detection (SAD) module and the Speaker Change Detection (SCD) module. We were unable to train the Speaker Embedding (EMB) module, but decided to test the full speaker diarization pipeline anyway. We found that the pipeline performed worse than the pipeline that used pretrained modules (SAD and SCD trained on the AMI Corpus, EMB trained on VoxCeleb1), but our SAD module achieved a DER percentage that was half of the DER from the pre-trained model. This shows that training pyannote-audio can lead to significant improvements, if the training works.

The main issue that we faced in Sprint 3 was getting the `lip-reading-deeplearning` library working on SCC. This library required multiple library installations that were problematic on SCC such as installing the ffmpeg video encoder library and CUDA, while also using Python libraries such as `dlib` and `opencv`. We ran into compatibility issues with using miniconda to install our Python libraries and homebrew to install ffmpeg, so we will be abandoning the usage of `lip-reading-deeplearning` for now. We will be moving to a library called [mouth-open](https://github.com/mauckc/mouth-open), which does not use the ffmpeg encoder. This software example uses facial landmarks (through `dlib` and `opencv`) to detect the ratio of the vertical and horizontal points of the mouth and gives a ratio score. We will experiment with the ratio value to find one that works best for our dataset.

After some deliberation, we narrowed down our dataset choices to `VoxCeleb` and `The AMI Corpus`. Both are free datasets that contain video of speakers and audio that contains utterance timestamps. In the end, we decided to go with `VoxCeleb` since the videos used were a collection of single and multiple faces, as opposed to `The AMI Corpus` videos which only had decent resolution vides of single faces. Additionally, `The AMI Corpus` dataset used a rather confusing annotation format that was not easily parsable. Using `VoxCeleb` is convenient for us, since we can then use the pretrained models from `pyannote-audio` that were pretrained on the `VoxCeleb1` subset. This will allow us to focus more on the implementation of the facial tracking system with speaker identification instead of on training `pyannote-audio`.

## Next Steps

One of the issues with `mouth-open` is that it appears to only work for one person in a frame. This is problematic for our system, since we expect our videos to contain two or more faces. We will research using `opencv` to perform multiple face detection and then run the facial landmark algorithm on the detected faces. Once we have this simple case working, we will explore further ways of applying machine learning to our facial landmarking tools so that we can improve the accuracy of our mouth tracking.

We will also be moving onto integrating the two systems into one application. We will be following the architecture we outlined in Sprint 2 and will create a more detailed architecture diagram as we finalize more details. 