# Team 6 - Sprint 2

## Summary

In Sprint 1, our team determined that multiple speaker identification/speaker diarization does not need to exist solely as an audio-only system. There is a demand for automatically generated transcripts for videos online as the number of videos and online streams continue to exponentially grow. Current Text-To-Speech (TTS) systems have a high level of accuracy, but most popular video platforms do not integrate speaker identities in their transcripts. Applying an audio-only speaker identification system on a video may work, but it is not the optimal solution. By combining audio and video information, we believe that we can increase the accuracy of speaker identification.

## Architecture

From an abstract, modular perspective, most speaker identification systems go through four major steps. These are voice activity detection, speaker embedding, speaker clustering, and resegmentation.

**Audio-only speaker identification system**:
![audio_only_system](https://raw.githubusercontent.com/gilbertyap/EC601-A2Team6-MultiSpeakerIdentification/master/Sprint2/audio_only_system.png)

In our proposed system, we want to leverage video data to improve the voice activity detection and speaker clustering portions of the audio-based speaker identification system. 

**Our proposed system**:
![team6_propose_system](https://raw.githubusercontent.com/gilbertyap/EC601-A2Team6-MultiSpeakerIdentification/master/Sprint2/team6_proposed_system.png)

This would be done by using the video data to help fine-tune the speaker segments generated by the voice activity detection process. Voice activity detection typically works by first extracting features from an audio signal and identifying the speech portions. Then, a sliding window is run through the signal and identifies segments were there are speech. The timestamps of these segments can be improved by recognizing when a speaker opens and closes their mouth. Once we have our final speaker segments and the audio-only speaker embeddings are created, we can again utilize the video data to aid the speaker clustering process. Since we know that only a number of unique mouths exist in the video, there should not be fewer identified speakers based on the audio. This information can help reduce the number of iterations needed in the clustering step.

We are also still deciding if our system should be able to give importance to audio or video data based on the signal-to-noise ratio (SNR) within a given audio-video stream. If there is too much noise within the audio signal, we could place more weight on the video's open/closed mouth segments. If ther video is not high quality (home video or on-site recording), we could place more weight on the audio system's determined speaker segments. We are still testing the effect of degraded audio and video quality on the final diarization error rate (DER).

## Technology Selection

There are three major components to our system: audio-based speaker identification system, video-based facial feature tracking system, and audio/video datasets.

The audio-based speaker identification system that was chosen for this project was [pyannote-audio](https://github.com/pyannote/pyannote-audio) for Python. "pyannote-audio" is a series of neural network blocks that make up a speaker diarization pipeline. The major benefit of using "pyannote-audio" is that each component of the pipeline, including the pipeline itself, is machine trainable. Compared to a system that can only train the overall pipeline, we should see compounding improvements in our system as we train each portion of the system on our datasets. "pyannote-audio" is regularly updated, and there are pull requests and tracked issues being discussed on their GitHub page dating back from the last few weeks and months.

The facial feature tracking system that was chosen was [lip-reading-deeplearning](https://github.com/astorfi/lip-reading-deeplearning). This software library was designed with lip-reading in mind, making it an excellent library for our speaker identification system. This library uses "Coupled 3D Convolutional Neural Networks" to synchronize mouth movements with speech audio. This done by first pre-processing the video file, and then running two deep neural network processes: speech feature extraction (using their own [SpeechPy](https://github.com/astorfi/speechpy) library), and mouth tracking. Since this library already performs a rudimentary amount of audio feature extraction, we may be able to leverage it in our system to reduce the amount of resources or time needed to process a video when used in combination with "pyannote-audio".

The datasets that we are investigating for our project are [VoxConverse](http://www.robots.ox.ac.uk/~vgg/data/voxconverse/) and [The AMI Corpus](http://groups.inf.ed.ac.uk/ami/corpus/). "VoxConverse" is comprised of > 50 hours of human conversations taken from public YouTube videos. The content of the videos are mostly political debates and broadcast news segements. One of the issues with this dataset is that the makeup of the speakers has not been identified, so it is difficult to determine the biases of the dataset without contacting the creators or listening to all of the audio clips. Another issue is that the links to each of the videos that were used is not provided (unlike the [VoxCeleb](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/) datasets). The "AMI Corpus" is >100 hours of meeting recordings (several different audio recording microphone types and video types) created by the AMI Project, which is based in Europe. Since there is a combination of audio and video data, we can determine the makeup of the dataset simply from examining the videos. Since the "AMI Corpus" provides many different types of audio recordings (lavalier microphone, far-field microphone, etc), we predict that providing such a wide range of information to "pyannote-audio" will allow it to predict speaker identities with higher accuracy compared to the higher-quality broadcast news recordings in VoxCeleb.

## User Story Demonstration

One of our user stories includes broadcast news reporters that are out in the field performing interviews or reports. While specific audio equipment can be chosen to reduce the amount of noise that ends up in the audio signal, not all noise is removed. We wanted to demonstrate the effect of noise on the DER in an audio-based speaker identification system. The details of this test can be found in the [audioOnlyTest](https://github.com/gilbertyap/EC601-A2Team6-MultiSpeakerIdentification/tree/master/Sprint2/audioOnlyTesting) folder within this `Sprint 2` folder. In summary, we found that as SNR increased, DER rates decreased but the pattern in which it did greatly depended on the audio signal rms values. We scaled all of the noise to have the same maximum amplitudes and found that not all SNR values had the same DER values. We may want to try conducting this experiment again with the SNR values normalized between each noise signal, but we believe that having "noise" signals with varying rms values more accurately represents "random" noise.

Our next test will involve performing a similar process of adding noise into the video signal and seeing how the accuracy of the lip reading system changes.

##  Resources

* [pyannote-audio](https://github.com/pyannote/pyannote-audio
* [lip-reading-deeplearning](https://github.com/astorfi/lip-reading-deeplearning)
* [VoxConverse](http://www.robots.ox.ac.uk/~vgg/data/voxconverse/)
* [The AMI Corpus](http://groups.inf.ed.ac.uk/ami/corpus/)
